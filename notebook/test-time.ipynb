{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1720832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6a0bb",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook benchmarks training time for one epoch on CIFAR-10 and CIFAR-100 datasets using a medium-complexity CNN model.\n",
    "\n",
    "**Model Architecture:**\n",
    "- 3 Convolutional blocks with BatchNorm and MaxPooling\n",
    "- Channel progression: 3 → 64 → 128 → 256\n",
    "- 2 Fully connected layers (4096 → 512 → num_classes)\n",
    "- Total parameters: ~3.5M for CIFAR-10, ~3.6M for CIFAR-100\n",
    "\n",
    "**Training Configuration:**\n",
    "- Batch size: 128\n",
    "- Optimizer: SGD with momentum (0.9) and weight decay (5e-4)\n",
    "- Learning rate: 0.01\n",
    "- Data augmentation: Random crop and horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed49fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([1, 10])\n",
      "Total parameters: 2,474,506\n"
     ]
    }
   ],
   "source": [
    "# Define a medium-complexity CNN model\n",
    "class MediumCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MediumCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1: 32x32x3 -> 32x32x64 -> 16x16x64\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16x64\n",
    "            \n",
    "            # Block 2: 16x16x64 -> 16x16x128 -> 8x8x128\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8x8x128\n",
    "            \n",
    "            # Block 3: 8x8x128 -> 8x8x256 -> 4x4x256\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 4x4x256\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 3 pooling layers: 32 -> 16 -> 8 -> 4\n",
    "        # Flattened size: 4 * 4 * 256 = 4096\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [batch_size, 4*4*256]\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Test model dimensions\n",
    "model_test = MediumCNN(num_classes=10)\n",
    "test_input = torch.randn(1, 3, 32, 32)\n",
    "test_output = model_test(test_input)\n",
    "print(f\"Model output shape: {test_output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model_test.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0605886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "CIFAR-10 training samples: 50000\n",
      "Number of batches: 391\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "data_root = Path.home() / 'data'\n",
    "batch_size = 128\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "cifar10_trainset = torchvision.datasets.CIFAR10(\n",
    "    root=str(data_root), \n",
    "    train=True, \n",
    "    download=False, \n",
    "    transform=transform_train\n",
    ")\n",
    "cifar10_trainloader = DataLoader(\n",
    "    cifar10_trainset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"CIFAR-10 training samples: {len(cifar10_trainset)}\")\n",
    "print(f\"Number of batches: {len(cifar10_trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e78099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CIFAR-100 dataset...\n",
      "CIFAR-100 training samples: 50000\n",
      "Number of batches: 391\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "print(\"\\nLoading CIFAR-100 dataset...\")\n",
    "cifar100_trainset = torchvision.datasets.CIFAR100(\n",
    "    root=str(data_root), \n",
    "    train=True, \n",
    "    download=False, \n",
    "    transform=transform_train\n",
    ")\n",
    "cifar100_trainloader = DataLoader(\n",
    "    cifar100_trainset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"CIFAR-100 training samples: {len(cifar100_trainset)}\")\n",
    "print(f\"Number of batches: {len(cifar100_trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62afd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train one epoch and measure time\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'  Batch [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / (batch_idx + 1):.3f}, '\n",
    "                  f'Acc: {100. * correct / total:.2f}%')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return elapsed_time, avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc180a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training on CIFAR-100 for ONE EPOCH\n",
      "============================================================\n",
      "  Batch [100/391], Loss: 4.326, Acc: 4.31%\n",
      "  Batch [200/391], Loss: 4.128, Acc: 6.64%\n",
      "  Batch [300/391], Loss: 4.017, Acc: 8.05%\n",
      "\n",
      "------------------------------------------------------------\n",
      "CIFAR-100 Results:\n",
      "  Time for 1 epoch: 6.38 seconds (0.11 minutes)\n",
      "  Average Loss: 3.9314\n",
      "  Training Accuracy: 9.32%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train on CIFAR-100 for one epoch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training on CIFAR-100 for ONE EPOCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_cifar100 = MediumCNN(num_classes=100).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_cifar100.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "elapsed_time, avg_loss, accuracy = train_one_epoch(\n",
    "    model_cifar100, \n",
    "    cifar100_trainloader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"CIFAR-100 Results:\")\n",
    "print(f\"  Time for 1 epoch: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"  Training Accuracy: {accuracy:.2f}%\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ed7d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training on CIFAR-10 for ONE EPOCH\n",
      "============================================================\n",
      "  Batch [100/391], Loss: 1.835, Acc: 31.87%\n",
      "  Batch [200/391], Loss: 1.705, Acc: 37.01%\n",
      "  Batch [300/391], Loss: 1.623, Acc: 40.10%\n",
      "\n",
      "------------------------------------------------------------\n",
      "CIFAR-10 Results:\n",
      "  Time for 1 epoch: 5.40 seconds (0.09 minutes)\n",
      "  Average Loss: 1.5741\n",
      "  Training Accuracy: 42.07%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train on CIFAR-10 for one epoch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training on CIFAR-10 for ONE EPOCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_cifar10 = MediumCNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_cifar10.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "elapsed_time, avg_loss, accuracy = train_one_epoch(\n",
    "    model_cifar10, \n",
    "    cifar10_trainloader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"CIFAR-10 Results:\")\n",
    "print(f\"  Time for 1 epoch: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"  Training Accuracy: {accuracy:.2f}%\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82435c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing with Pretrained ResNet-18\n",
      "============================================================\n",
      "\n",
      "--- CIFAR-10 with ResNet-18 ---\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/hli54/.cache/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 290MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch [100/391], Loss: 1.432, Acc: 49.01%\n",
      "  Batch [200/391], Loss: 1.217, Acc: 57.48%\n",
      "  Batch [300/391], Loss: 1.105, Acc: 61.65%\n",
      "\n",
      "------------------------------------------------------------\n",
      "ResNet-18 on CIFAR-10 Results:\n",
      "  Time for 1 epoch: 5.70 seconds (0.10 minutes)\n",
      "  Average Loss: 1.0350\n",
      "  Training Accuracy: 64.19%\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- CIFAR-100 with ResNet-18 ---\n",
      "  Batch [100/391], Loss: 3.619, Acc: 16.06%\n",
      "  Batch [200/391], Loss: 3.214, Acc: 22.45%\n",
      "  Batch [300/391], Loss: 2.987, Acc: 26.41%\n",
      "\n",
      "------------------------------------------------------------\n",
      "ResNet-18 on CIFAR-100 Results:\n",
      "  Time for 1 epoch: 5.54 seconds (0.09 minutes)\n",
      "  Average Loss: 2.8452\n",
      "  Training Accuracy: 29.09%\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Model Comparison:\n",
      "  Custom MediumCNN parameters: 2,474,506\n",
      "  ResNet-18 parameters: 11,181,642\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-18 and test training time\n",
    "import os\n",
    "os.environ['TORCH_HOME'] = str(Path.home() / '.cache')\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing with Pretrained ResNet-18\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test on CIFAR-10\n",
    "print(\"\\n--- CIFAR-10 with ResNet-18 ---\")\n",
    "resnet_cifar10 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Modify final layer for CIFAR-10 (10 classes)\n",
    "resnet_cifar10.fc = nn.Linear(resnet_cifar10.fc.in_features, 10)\n",
    "resnet_cifar10 = resnet_cifar10.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet_cifar10.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "elapsed_time, avg_loss, accuracy = train_one_epoch(\n",
    "    resnet_cifar10,\n",
    "    cifar10_trainloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"ResNet-18 on CIFAR-10 Results:\")\n",
    "print(f\"  Time for 1 epoch: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"  Training Accuracy: {accuracy:.2f}%\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Test on CIFAR-100\n",
    "print(\"\\n--- CIFAR-100 with ResNet-18 ---\")\n",
    "resnet_cifar100 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Modify final layer for CIFAR-100 (100 classes)\n",
    "resnet_cifar100.fc = nn.Linear(resnet_cifar100.fc.in_features, 100)\n",
    "resnet_cifar100 = resnet_cifar100.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet_cifar100.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "elapsed_time, avg_loss, accuracy = train_one_epoch(\n",
    "    resnet_cifar100,\n",
    "    cifar100_trainloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"ResNet-18 on CIFAR-100 Results:\")\n",
    "print(f\"  Time for 1 epoch: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"  Training Accuracy: {accuracy:.2f}%\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Compare model sizes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"  Custom MediumCNN parameters: {sum(p.numel() for p in MediumCNN(10).parameters()):,}\")\n",
    "print(f\"  ResNet-18 parameters: {sum(p.numel() for p in resnet_cifar10.parameters()):,}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
